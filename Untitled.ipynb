{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from numpy.fft import fft, ifft\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from random import uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Holographic Embedding Implemetations\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_entity,num_rel,emb_dim):\n",
    "        super(TransE,self).__init__()\n",
    "        self.ent_embeddings=nn.Embedding(num_entity,emb_dim)\n",
    "        self.rel_embeddings=nn.Embedding(num_rel,emb_dim)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.ent_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.rel_embeddings.weight.data)\n",
    "    \n",
    "    def _calc(self,h,t,r):\n",
    "        return torch.abs(h + r - t)\n",
    "    \n",
    "    \n",
    "    # margin-based loss\n",
    "    def loss_func(self,p_score,n_score):\n",
    "        criterion = nn.MarginRankingLoss(1.)\n",
    "        y = Variable(torch.Tensor([-1]))\n",
    "        loss = criterion(p_score,n_score,y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self,pos_inputs,neg_inputs):\n",
    "        # [batch_size]\n",
    "        pos_h = pos_inputs[:,0]\n",
    "        pos_t = pos_inputs[:,1]\n",
    "        pos_r = pos_inputs[:,2]\n",
    "        # [batch_size,num_neg_sample]\n",
    "        neg_h = neg_inputs[:,:,0]\n",
    "        neg_t = neg_inputs[:,:,1]\n",
    "        neg_r = neg_inputs[:,:,2]\n",
    "\n",
    "        # [batch_size,embedding_size]\n",
    "        pos_h_embed = self.ent_embeddings(pos_h)\n",
    "        pos_t_embed = self.ent_embeddings(pos_t)\n",
    "        pos_r_embed = self.rel_embeddings(pos_r)\n",
    "        # [batch_size,num_neg_sample,embedding_size]\n",
    "        neg_h_embed = self.ent_embeddings(neg_h)#.view(Batch_Size,-1,emb_dim)\n",
    "        #print neg_h_embed.size()\n",
    "        #print neg_h.size()\n",
    "        neg_t_embed = self.ent_embeddings(neg_t)#.view(Batch_Size,-1,emb_dim)\n",
    "        neg_r_embed = self.rel_embeddings(neg_r)#.view(Batch_Size,-1,emb_dim)\n",
    "        pos_score = self._calc(pos_h_embed,pos_t_embed,pos_r_embed)\n",
    "        neg_score = self._calc(neg_h_embed,neg_t_embed,neg_r_embed)\n",
    "        #print pos_score\n",
    "        #print neg_score\n",
    "        #print torch.mean(pos_score,1).size()\n",
    "        #print pos_score.size()\n",
    "        #print neg_score.size()\n",
    "        #print torch.mean(neg_score, 2).size()\n",
    "        pos_score = torch.sum(pos_score,1)\n",
    "        #print torch.mean(neg_score, 1)\n",
    "        neg_score = torch.sum(torch.mean(neg_score, 1),1)\n",
    "        #print pos_score\n",
    "        #print neg_score\n",
    "        #print neg_score.size()\n",
    "        #print pos_score.size()\n",
    "        #neg_score = torch.mean(neg_score,1)\n",
    "        loss = self.loss_func(pos_score,neg_score)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, predict_h, predict_t, predict_r):\n",
    "        pred_h = self.ent_embeddings(Variable(torch.from_numpy(predict_h)))\n",
    "        pred_t = self.ent_embeddings(Variable(torch.from_numpy(predict_t)))\n",
    "        pred_r = self.rel_embeddings(Variable(torch.from_numpy(predict_r)))\n",
    "        p_score  = self._calc(pred_h.view(-1,emb_dim),pred_t.view(-1,emb_dim),pred_r.view(-1,emb_dim))\n",
    "        p_score = torch.sum(p_score,1)\n",
    "        return p_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the model\n",
    "Total_Entities = 400\n",
    "Total_Relations = 4\n",
    "num_entity = Total_Entities\n",
    "num_relation = Total_Relations\n",
    "emb_dim = 15\n",
    "\n",
    "transe = TransE(num_entity,num_relation,emb_dim)\n",
    "\n",
    "checkpoint = torch.load(\"./Data/transe_model.pth.tar\",map_location=lambda storage, loc: storage)\n",
    "transe.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def transe_prediction(model,fact):\n",
    "    model.eval()\n",
    "    i = fact['entity_a']\n",
    "    j = fact['entity_b']\n",
    "    score = 100000\n",
    "    for k in range(4):\n",
    "        #xt = [i,j,k]\n",
    "        i = np.array([[i]])\n",
    "        j = np.array([[j]])\n",
    "        k = np.array([[k]])\n",
    "        pred_score = model.predict(i,j,k)\n",
    "        #print pred_score\n",
    "        if score > pred_score.data:\n",
    "            prediction = k\n",
    "            score = pred_score.data\n",
    "    return score, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade1f6295a014223a1516d168533890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaurya/anaconda2/lib/python2.7/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- - - - - - - - - - - - - STATISTICS ON TRAINING DATASET - - - - - - - - - - - - - - \n",
      "\n",
      "Total number of Green links in training dataset : 384 \n",
      " Number of Green predicted correctly: 383\n",
      " Accuracy of Green predicted correctly: 0.997395833333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a03b5f9c45d4b28b7288e9e495b9aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- - - - - - - - - - - - - STATISTICS ON VALIDATION DATASET - - - - - - - - - - - - - - \n",
      "\n",
      "Total number of Green links in training dataset : 126 \n",
      " Number of Green predicted correctly: 88\n",
      " Accuracy of Green predicted correctly: 0.698412698413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0862980c703744c28781713cc6fc4cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- - - - - - - - - - - - - STATISTICS ON TO_USE DATASET - - - - - - - - - - - - - - \n",
      "\n",
      "Total number of Green links in training dataset : 381 \n",
      " Number of Green predicted correctly: 266\n",
      " Accuracy of Green predicted correctly: 0.698162729659\n"
     ]
    }
   ],
   "source": [
    "## Training Data Stats\n",
    "\n",
    "train_data =  pd.read_csv('./Data/train.csv', delimiter=',')\n",
    "train_data.dropna(inplace=True)\n",
    "train_data['transe_prediction'] = ''\n",
    "# Calculating train data stats\n",
    "for idx,dat in  tqdm.tqdm_notebook(enumerate(train_data.iterrows())):\n",
    "    _,train_data['transe_prediction'].iloc[idx] = transe_prediction(transe,dat[1])\n",
    "\n",
    "acc_count = (train_data['transe_prediction'] == train_data['relation']).sum()\n",
    "acc = float(acc_count)/train_data.shape[0]\n",
    "\n",
    "# Printing results\n",
    "print(\"\\n- - - - - - - - - - - - - STATISTICS ON TRAINING DATASET - - - - - - - - - - - - - - \\n\")\n",
    "print(\"Total number of Green links in training dataset : {} \".format(train_data.shape[0]))\n",
    "print(\" Number of Green predicted correctly: {}\".format(acc_count))\n",
    "print(\" Accuracy of Green predicted correctly: {}\".format(acc))\n",
    "\n",
    "## Validation Data Stats\n",
    "\n",
    "valid_data =  pd.read_csv('./Data/valid.csv', delimiter=',')\n",
    "valid_data.dropna(inplace=True)\n",
    "valid_data['transe_prediction'] = ''\n",
    "# Calculating train data stats\n",
    "for idx,dat in  tqdm.tqdm_notebook(enumerate(valid_data.iterrows())):\n",
    "    _,valid_data['transe_prediction'].iloc[idx] = transe_prediction(transe,dat[1])\n",
    "\n",
    "acc_count = (valid_data['transe_prediction'] == valid_data['relation']).sum()\n",
    "acc = float(acc_count)/valid_data.shape[0]\n",
    "\n",
    "# Printing results\n",
    "print(\"\\n- - - - - - - - - - - - - STATISTICS ON VALIDATION DATASET - - - - - - - - - - - - - - \\n\")\n",
    "print(\"Total number of Green links in training dataset : {} \".format(valid_data.shape[0]))\n",
    "print(\" Number of Green predicted correctly: {}\".format(acc_count))\n",
    "print(\" Accuracy of Green predicted correctly: {}\".format(acc))\n",
    "\n",
    "## To_use Data Stats\n",
    "\n",
    "test_data =  pd.read_csv('./Data/to_use.csv', delimiter=',')\n",
    "test_data.dropna(inplace=True)\n",
    "test_data['transe_prediction'] = ''\n",
    "# Calculating train data stats\n",
    "for idx,dat in  tqdm.tqdm_notebook(enumerate(test_data.iterrows())):\n",
    "    _,test_data['transe_prediction'].iloc[idx] = transe_prediction(transe,dat[1])\n",
    "\n",
    "acc_count = (test_data['transe_prediction'] == test_data['relation']).sum()\n",
    "acc = float(acc_count)/test_data.shape[0]\n",
    "\n",
    "# Printing results\n",
    "print(\"\\n- - - - - - - - - - - - - STATISTICS ON TO_USE DATASET - - - - - - - - - - - - - - \\n\")\n",
    "print(\"Total number of Green links in training dataset : {} \".format(test_data.shape[0]))\n",
    "print(\" Number of Green predicted correctly: {}\".format(acc_count))\n",
    "print(\" Accuracy of Green predicted correctly: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "clf_from_joblib = joblib.load('filename.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
